# 4. Spark in data science
Data scientists performing investigative analytics use interactive statistical environments to perform ad-hoc, exploratory analytics in order to answer questions and gain insights. By contrast, data scientists building operational analytics systems have more in common with engineers. They build software that creates and queries machine-learning models that operate at scale in real-time serving environments, using systems languages like C++ and Java.
While discussion about Spark for data science has mostly noted its ability to keep data resident in memory, which can speed up iterative machine learning workloads compared to MapReduce. It does not solve every problem for everyone. However, Spark has a number of features (discribed in [2.2 Ecosystem](#2.2-ecosystem)) that make it a compelling crossover platform for investigative as well as operational analytics. With all these features, Spark has become the center of attraction for almost all of the Big Data developers and Data scientists. Though it has only been a few years, Spark has been evolving quickly and promises to be a sure contender for an industry standard in Big Data.
Data scientists, in many cases need to run their algorithms right on top of the data. In case of Big Data they need capability to run their code over data in a "massive" parallel manner - otherwise it will be too slow for their research purposes. Spark can be viewed as very convenient way to write code which will run in parallel over data and assemble the results.

