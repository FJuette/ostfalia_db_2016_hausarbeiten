{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf760
{\fonttbl\f0\fnil\fcharset0 Vollkorn-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;}
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\fi366\pardirnatural

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
Das Hadoop System ist die Infrastruktur zum handhaben dieser Datenmengen. Zur Analyse dieser Daten k\'f6nnte dann, wie ebenfalls aus den Anforderungen der Stellenbeschreibung hervorgeht, das Apache Spark Framework verwendet werden. Mit Spark k\'f6nnen Daten aus einer vielen verschiedenen Quellen verarbeitet werden. Zum Beispiel auch aus dem HDFS, aber auch aus NoSQL Datenbank Systemen oder relationalen  Datenbanken. Mit Spark k\'f6nnen Daten aus dem HDFS schneller verarbeitet werden und es existieren flexiblere Alternativen zum Hadoop MapReduce Verfahren.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\fi366\pardirnatural
{\field{\*\fldinst{HYPERLINK "scrivcmt://508D63D1-C92F-4C8C-82A7-EE3E089FBC34"}}{\fldrslt \cf2 \cb3 MapReduce ist ein Verfahren welches von Google f\'fcr gro\'dfe Datenmengen entworfen wurde. Map: Daten werden \'fcber einen Cluster von Rechner verteilt nach einer bestimmten Funktion abgearbeitet. Reduce: Die einzelnen Teile des Clusters liefern nur einen Wert zur\'fcck.}} {\field{\*\fldinst{HYPERLINK "scrivcmt://C9CBE5AE-2C22-4E12-B2F0-6D4C533CF2E3"}}{\fldrslt \cf2 \cb3 Spark kommt geb\'fcndelt mit einer Bibliothek f\'fcr maschinelles Lernen (MLib), was immer iterative Prozesses bedeutet, hat eine REPL (Read Eval Print Listen) Schnittstelle und kann \'e4hnlich wir R oder Python explorativ f\'fcr statistische Aufgaben verwendet werden.}} Seit Anfang 2014 gilt Spark als {\field{\*\fldinst{HYPERLINK "scrivcmt://FD6ACECA-A542-402D-AEB0-638A8D6D6119"}}{\fldrslt \cf2 \cb3 Top Level Project bei der Apache Foundation}}.  \
Wenn Datenmengen bearbeitet werden sollen, die auf einer einzigen Maschine existieren und verarbeitet werden k\'f6nnen, kommt auch gerne die Programmiersprache Python, in ihren interaktiven Umgebungen wie {\field{\*\fldinst{HYPERLINK "scrivcmt://F898A827-9B11-4171-AF6E-22F4B3F7508D"}}{\fldrslt iPython}} oder {\field{\*\fldinst{HYPERLINK "scrivcmt://416896A6-5E06-435B-9DFB-DB1B110AF36C"}}{\fldrslt Jupyter}}, mit Paketen wie {\field{\*\fldinst{HYPERLINK "scrivcmt://C35AAE4D-E981-44E9-9D86-424A4C789F29"}}{\fldrslt scikit-learn}} oder {\field{\*\fldinst{HYPERLINK "scrivcmt://E733F774-EBAA-4282-9057-D30F8944B769"}}{\fldrslt pandas}} zum Einsatz. Pandas zum bearbeiten von Datenstrukturen und scikit-learn zum analysieren von Daten. Weiter \'fcbliche Module sind {\field{\*\fldinst{HYPERLINK "scrivcmt://376C176E-EA58-410A-B077-1FD5367DADB0"}}{\fldrslt Matplotlib}} f\'fcr die Ausgabe als Plot}