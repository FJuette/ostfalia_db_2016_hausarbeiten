{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf760
{\fonttbl\f0\fnil\fcharset0 Vollkorn-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;}
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\fi366\pardirnatural

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
Wenn die Behauptung der Firma CA stimmt ist es offensichtlich, dass ein relationale Datenbank nicht ausreicht um diese 4000 Datenpunkte zu speichern. Auch die Analyse kann nicht auf einer einzelnen Maschine stattfinden. {\field{\*\fldinst{HYPERLINK "scrivcmt://5076CC3D-7104-4E8D-86B3-886CFB648DB8"}}{\fldrslt \cf2 \cb3 Hier kommt das Hadoop Distributed File System (HDFS) zum Einsatz. Hadoop ist ein Open Source (Java) Projekt, welches von Googles propriet\'e4ren Google File System (GFS) und dem MapReduce Framework inspiriert wurde. Mit diesem System k\'f6nnen sehr gro\'dfe Datens\'e4tze zuverl\'e4ssig gespeichert werden und mit hoher Bandbreite an Anwendungen, sogenannte HDFS Clients, \'fcbertragen werden. Die HDFS Architektur besteht aus einem einzelnen NameNode, vielen DataNodes und dem HDFS Client. Der NameNode organisiert die Anfragen und die Ablage von Daten von den DataNodes durch die Clients. Das Hadoop MapReduce Framework wurde entworfen um Speicherung- und Berechnungsaufgaben \'fcber viele tausend Server zu verteilen und bei Bedarf zu skalieren.}} }