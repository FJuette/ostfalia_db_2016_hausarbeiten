# Fazit
Wie eingangs beschrieben, wird im CAP-Theorem abgebildet, dass die Aspekte Konsistenz, Verfügbarkeit und Ausfalltoleranz nicht gleichzeitig vorhanden sein können [5]. Lediglich maximal zwei der drei Bedingungen können gleichzeitig erfüllt werden [6].

Damit beschreibt das CAP-Theorem einen durchaus wichtigen Sachverhalt, auch wenn das Prinzip – falls streng ausgelegt – an seine Grenzen gerät. Es ist aus dem Grund von großem Interesse, weil es eine Beschreibung der Notwendigkeiten des Abwägens vor allem zwischen den Anforderungen „C“ und „A“ darstellt, sowie die möglichen Lösungswege aufzeigt, um für verschiedene Anwendungen passende Kompromisse zu finden. Dies kann bereits schon bei der Wahl des Datenbankmanagementsystems von Bedeutung sein. Viele verteilte Datenbanken lassen es zu, dass selbst für einzelne Operationen innerhalb einer Datenbank mit verschiedenen Strategien zur Sicherung der Konsistenz gearbeitet werden kann. Damit ist es möglich innerhalb von einzelnen Applikationen je nach Fall auch eine andere Konstellation der Restriktionen im Rahmen des CAP-Theorems auszuwählen [16].

System-Designer sollten nicht blind die Konsistenz oder Verfügbarkeit opfern, wenn Partitionen vorhanden sind. Mit dem jeweils passenden Ansatz können sie beide Eigenschaften durch sorgfältige Verwaltung von Invarianten bei Partitionen optimieren. Da neuere Techniken, wie Versionsvektoren und der sogenannte „conflict-free replicated data type“ (CRDT), in Frameworks übergehen, die ihre Verwendung vereinfachen, sollte diese Art der Optimierung weiter verbreitet werden. Im Gegensatz zu ACID-Transaktionen erfordert dieser Ansatz jedoch eine stärker umsichtige Implementierung im Vergleich zu früheren Strategien. Die besten Lösungen hängen hierbei stark von Details im Rahmen der Invarianten und Operationen des Services ab [12].

Das CAP-Theorem stellt ein gutes Beispiel dar, für eine mögliche Art der Herangehensweise, um einen grundlegenden Kompromiss zwischen Sicherheit und Lebendigkeit in fehleranfälligen Systemen zu finden. Die Untersuchung dieses inhärenten Kompromisses gibt Auskunft darüber, wie Systeme trotz unzuverlässiger Netzwerke auf die Bedürfnisse einer Applikation abgestimmt werden können. Demnach haben Softwarearchitekten mit bestmöglicher Verfügbarkeit sehr konsistente Lösungen erforscht, sie haben schwach konsistente Lösungen mit hoher Verfügbarkeit erforscht und sie haben Systeme untersucht, die sowohl schwächere Verfügbarkeit als auch schwächere Konsistenz in unterschiedlicher Weise mischen. Gleichzeitig hat sich die vernetzte Welt in den vergangenen zehn Jahren entscheidend verändert und damit neue Herausforderungen für Systementwickler mit sich gebracht, sowie neue Bereiche geschaffen, in denen diese inhärenten Kompromisse erforscht werden können. Es werden neue theoretische Einsichten benötigt, um diese Herausforderungen zu bewältigen. Ebenso werden neue Techniken zur Bewältigung des Problems in realen Systemen notwendig [17].

In zunehmendem Maße wird gefordert, dass unsere Systeme skalierbar sind, nicht nur für die Kunden von heute, sondern auch für das Wachstum von morgen. Intuitiv denken wir dabei an ein System als skalierbar, wenn es effizient wachsen kann, indem wir neue Ressourcen effizient nutzen, um die steigende Last zu bewältigen. Es scheint inhärente Kompromisse zwischen Skalierbarkeit und Konsistenz zu geben. Um beispielsweise neue Ressourcen effizient nutzen zu können, muss eine Koordination dieser Ressourcen erfolgen. Die für diese Koordination erforderliche Kohärenz unterliegt den Kompromissen des CAP-Theorems. Die Untersuchung dieser Frage kann helfen, zu erklären, warum selbst innerhalb eines Rechenzentrums, wo es nur selten Partitionen gibt, es schwierig erscheint, stark konsistente Protokolle effizient zu skalieren [17].

Des Weiteren konzentriert sich das CAP-Theorem auf Netzwerkpartitionen. Manche Server können nicht zuverlässig kommunizieren. In zunehmendem Maße werden allerdings immer stärkere Angriffe auf Netzwerke beobachtet. Zum Beispiel werden Denial-of-Service-Angriffe (Verweigerung des Dienstes) eine nahezu kontinuierliche Bedrohung für den täglichen Netzwerkbetrieb. Ein Denial-of-Service-Angriff kann jedoch nicht einfach als Netzwerkpartition modelliert werden. Ebenso werden Probleme mit böswilligen Nutzern beobachtet, die Server hacken und sonstige Internetdienste stören. Die Verträglichkeit dieser problematischeren Formen der Störung erfordert ein etwas anderes Verständnis der grundlegenden Kompromisse zwischen Konsistenz und Verfügbarkeit [17].

Das CAP-Theorem konzentrierte sich in der Vergangenheit zunächst auf weiträumige Internetdienste. Heute jedoch wird ein signifikanter und zunehmender Prozentsatz des Internet-Traffics von mobilen Geräten initiiert. Viele der Kompromisse, die im Rahmen des CAP-Theorems erforscht werden, sind übertragbar auf die der Mobilfunknetze und viele der Probleme sind noch schwerer zu lösen. Bemerkenswerterweise ist drahtlose Kommunikation notorisch unzuverlässig. Das Hauptproblem des CAP-Theorems war die Häufigkeit halbstabiler Partitionen, die sich alle paar Minuten ändern. In einem drahtlosen Netzwerk sind Partitionen weniger häufig. Jedoch ist in diesem Fall ein unvorhersehbarer Nachrichtenverlust sehr häufig und die jeweilige Nachrichtenlatenz kann signifikant variieren [17].

Darüber hinaus können die Arten von Anwendungen, die in drahtlosen Netzwerken eingesetzt werden, etwas anders sein. Das CAP-Theorem wurde von Internet-Suchmaschinen und E-Commerce-Websites motiviert. Es gibt eine neue Generation von drahtlosen Anwendungen, die sich jedoch auf unterschiedliche Prioritäten konzentrieren. Demnach sind Geographie und Nähe kritisch, soziale Interaktionen sind primär und die Privatsphäre hat eine unmittelbare Bedeutung. Ein Beipspiel stellt „foursquare“ dar, eine Anwendung, in der Benutzer sich an bestimmten Standorten einloggen und Kommentare und Diskussion beginnen, bezogen darauf, wo sie sich gerade befinden. Zukünftig sollte eine erneute Überprüfung des CAP-Theorems im Zusammenhang mit drahtlosen Netzwerken stattfinden, um die besonderen Kompromisse, die in diesem Zusammenhang auftreten, besser zu verstehen [17].

Naheliegend ist außerdem, dass strenge Forderungen der Konsistenz, wie sie im ACID-Prinzip gefordert werden, immer auch Einbußen in der Performance und Verfügbarkeit innerhalb verteilter Systeme mit sich bringen. Ebenso führen hohe Anforderungen an die Performance und an die Verfügbarkeit, was international in verteilten Systemen üblich ist und mit Hilfe des BASE-Prinzips umgesetzt wird, zu Einbußen in Bezug auf die Konsistenz. Diese Aspekte wurden mit dem CAP-Theorem vermutet und letztendlich auch bewiesen [11].

Google hat sich nun mit der neuen weltweiten Spanner Datenbank vorgenommen, nachzuweisen, dass C, A und P eventuell unter gewissen Bedingungen zumindest besser als zuvor zur gleichen Zeit erfüllt werden können [11].

In der NoSQL-Community wird indessen phantasiereich und lebhaft diskutiert und entwickelt. Dabei stellt die „freie Datenbankwahl“ einen enorm wichtigen Punkt dar, was dem Trend der Langzeit-Exklusiv-Verträge entgegensteuert. Zukünftig ist zu erwarten, dass es aufgrund neuer Entwicklungen im NoSQL-Bereich zunehmend notwendig sein wird, vermehrt Zeit in die Analyse bei der Auswahl eines Datenmodells zu investieren. Dabei muss eingehend untersucht werden, welches Modell für welche Aufgabenstellung als sinnvoll zu erachten ist. Dabei gibt es eine Vielfalt an NoSQL-Modellen, sodass für nahezu jede Problemstellung ein passendes DBS entwickelt werden kann [10].

Letztendlich stellt NoSQL eine sinnvolle Erweiterung der Datenbankwelt dar und ist keine Herausforderung für die Konstrukteure in der relationalen Welt. Mit ständig steigenden Anforderungen entstehen immer wieder neue NoSQL-Systeme und –Modelle [10].

Das CAP-Theorem war insbesondere äußerst nützlich, um Konstrukteuren dabei zu helfen, mit Hilfe vorgeschlagener System-Fähigkeiten zu argumentieren und den übertriebenen Marketing-Hype vieler kommerzieller Datenbankmanagementsysteme aufzudecken. Seit seinem ursprünglichen formalen Nachweis ist das CAP-Theorem jedoch zunehmend missverstanden und falsch angewandt worden, was möglicherweise erheblichen Schaden verursacht hat. Insbesondere schließen viele Konstrukteure fälschlicherweise daraus, dass der Satz bestimmte Beschränkungen für ein Datenbankmanagementsystem während des normalen Systembetriebs auferlegt und daher ein unnötig begrenztes System implementiert wird. In Wirklichkeit legt das CAP-Theorem allerdings nur Einschränkungen im Hinblick auf bestimmte Arten von Ausfällen fest und schränkt keine Systemfähigkeiten während des normalen Betriebs ein [19].
