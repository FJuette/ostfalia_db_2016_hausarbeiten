# 3. Spark and Hadoop
Hadoop as a well-known big data processing technology which has been around for ten years and has proven to be the solution of choice for processing large data sets. MapReduce is a great solution for *one-pass* computations, but not very efficient for use cases that require *multi-pass* computations and algorithms.
A multi-pass computation is an algorithm that processes the code or data several times. This is in contrast to a one-pass computation, which processes the data only once. Each pass takes the result of the previous pass as an input, and creates an intermediate output. In this way, the intermediate output is improved pass by pass, until the final pass emits the final result. Each step in the data processing workflow has one Map phase and one Reduce phase and you will need to convert any use case into MapReduce pattern to leverage this solution.

The output data between each step has to be stored in a distributed file system before the next step can begin. Hence, this approach tends to be slow due to replication and disk storage. Also, Hadoop solutions typically include clusters that are hard to set up and manage. It also requires the integration of several tools like Storm and Machine Learning for different Big Data use cases.
If you wanted to do something complicated, you would have to string together a series of MapReduce jobs and execute them in sequence. Each of those jobs are high-latency, and none could start until the previous job had finished completely.

On the contrary Spark allows developers to create complex, multi-step data pipelines using Directed Acyclic Graph (DAG) pattern. It also supports in-memory data sharing across DAGs, so that different jobs can work with the same data.
A DAG is a finite directed graph with no directed cycles. That is, it consists of finitely many vertices and edges, with each edge directed from one vertex to another, such that there is no way to start at any vertex *v* and follow a consistently-directed sequence of edges that eventually loops back to *v* again. Equivalently, a DAG is a directed graph that has a topological ordering, a sequence of the vertices such that every edge is directed from earlier to later in the sequence.

Spark runs on top of existing *Hadoop Distributed File System* (HDFS) infrastructure to provide enhanced and additional functionality. It provides support for deploying Spark applications in an existing Hadoop v1 cluster with Spark-Inside-MapReduce (SIMR) or Hadoop v2 YARN cluster or even Apache Mesos.

Hadoop MapReduce and Spark could run on the same hardware, but when it comes to the costs there are slightly a difference between those two. MapReduce uses standard amounts of memory because its processing is disk-based, so a company will have to pay for faster disks and a lot of disk space to run MapReduce. MapReduce also requires more systems to distribute the disk I/O over multiple systems.
Spark requires a lot of memory (RAM), but does not need much disk space. Spark does store some temporary files on the disk, but typically these files are kept for seven days just to speed up any processing on the same data sets. Disk space nowadays is a relatively inexpensive asset and since Spark does not use disk I/O for processing, the disk space used can be a *SAN* (Storage Area Network) or *NAS* (Network Attached Storage).
However the Spark systems cost more because of the large amounts of RAM required to run processes in memory. But on the other hand Sparkâ€™s technology reduces the number of systems needed. That means you have significantly less systems that cost more. There is probably a point at which Spark actually reduces costs per unit of computation even with the additional RAM requirement.

Spark should be considered as an alternative to Hadoop MapReduce rather than a replacement to Hadoop. It is not intended to replace Hadoop, but to provide a comprehensive and unified solution to manage different Big Data use cases and requirements.
